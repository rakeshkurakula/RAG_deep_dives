{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing the RAG performance\n",
    "- better retrievar\n",
    "- better computational load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset preparation\n",
    "- Using Stanford Q and A dataset (SQuAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install datasets\n",
    "# %pip install einops\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# loading the dataset\n",
    "dataset = load_dataset(\"squad\")\n",
    "\n",
    "# Extract unique context from the dataset\n",
    "\n",
    "data = [item[\"context\"] for item in dataset[\"train\"]]\n",
    "\n",
    "texts = list(set(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The United States is the chief remaining nation to assign official responsibilities to a region called the Near East. Within the government the State Department has been most influential in promulgating the Near Eastern regional system. The countries of the former empires of the 19th century have in general abandoned the term and the subdivision in favor of Middle East, North Africa and various forms of Asia. In many cases, such as France, no distinct regional substructures have been employed. Each country has its own French diplomatic apparatus, although regional terms, including Proche-Orient and Moyen-Orient, may be used in a descriptive sense. The most influential agencies in the United States still using Near East as a working concept are as follows.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed dataset\n",
    "Embedding for each context-level, Each element of the above text list will be embedded into a single vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from tqdm import tqdm\n",
    "def batch_iterate(lst, batch_size):\n",
    "    for i in range(0, len(lst), batch_size):\n",
    "        yield lst[i : i + batch_size]\n",
    "    \n",
    "class EmbedData:\n",
    "    \"\"\"\n",
    "    A class for generating and managing text embeddings using a Hugging Face embedding model.\n",
    "    This class handles the loading of an embedding model and batch processing of text data\n",
    "    to generate embeddings.\n",
    "    Attributes:\n",
    "        embed_model_name (str): Name of the Hugging Face model to use for embeddings.\n",
    "            Defaults to \"nomic-ai/nomic-embed-text-v1.5\".\n",
    "        embed_model: Loaded Hugging Face embedding model instance.\n",
    "        batch_size (int): Number of texts to process in each batch. Defaults to 32.\n",
    "        embeddings (list): Storage for generated embeddings.\n",
    "    Example:\n",
    "        >>> embed_data = EmbedData()\n",
    "        >>> texts = [\"Sample text 1\", \"Sample text 2\"]\n",
    "        >>> embed_data.embed(texts)\n",
    "        >>> embeddings = embed_data.embeddings\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 embed_model_name=\"nomic-ai/nomic-embed-text-v1.5\",\n",
    "                 batch_size = 32):\n",
    "        self.embed_model_name = embed_model_name\n",
    "        self.embed_model = self._load_embed_model()\n",
    "        self.batch_size = batch_size\n",
    "        self.embeddings = []\n",
    "\n",
    "    def _load_embed_model(self):\n",
    "        \"\"\"\n",
    "        Load and initialize a HuggingFace embedding model with specified configurations.\n",
    "\n",
    "        Returns:\n",
    "            HuggingFaceEmbedding: Initialized embedding model instance configured with the model name.\n",
    "        \"\"\"\n",
    "        embed_model = HuggingFaceEmbedding(model_name=self.embed_model_name,\n",
    "                                           trust_remote_code=True,\n",
    "                                           cache_folder='./hf_cache')\n",
    "        return embed_model\n",
    "    \n",
    "    def generate_embedding(self, context):\n",
    "        return self.embed_model.get_text_embedding_batch(context)\n",
    "    \n",
    "\n",
    "    def embed(self, contexts):\n",
    "        \"\"\"\n",
    "        Embeds a list of contexts into vector representations using batched processing.\n",
    "        This method processes the input contexts in batches and generates embeddings \n",
    "        for each context using the underlying embedding model. The embeddings are stored\n",
    "        internally in the class instance.\n",
    "        Args:\n",
    "            contexts (list): List of text contexts to be embedded.\n",
    "                             Each context should be a string.\n",
    "        Example:\n",
    "            embedder = EmbeddingModel()\n",
    "            contexts = [\"text1\", \"text2\", \"text3\"]\n",
    "            embedder.embed(contexts)\n",
    "        \"\"\"\n",
    "        self.contexts = contexts\n",
    "        \n",
    "        for batch_context in tqdm(batch_iterate(contexts, self.batch_size),\n",
    "                                  total=len(contexts)//self.batch_size,\n",
    "                                  desc=\"Embedding data in batches\"):\n",
    "                                  \n",
    "            batch_embeddings = self.generate_embedding(batch_context)\n",
    "            \n",
    "            self.embeddings.extend(batch_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rakeshk94/miniconda3/envs/torch/lib/python3.9/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "<All keys matched successfully>\n",
      "Embedding data in batches:  27%|██▋       | 160/590 [05:35<21:48,  3.04s/it]"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "embeddata = EmbedData(batch_size=batch_size)\n",
    "\n",
    "embeddata.embed(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Writing the embedings to pickle file\n",
    "import pickle\n",
    "with open(\"data/squad_embedded_full.pickle\", \"wb\") as h:\n",
    "    pickle.dump(embeddata,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers_modules.nomic-ai.nomic-bert-2048.c1b1fd7a715b8eb2e232d34593154ac782c98ac9'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdill\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/Embed Data Full.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m h:\n\u001b[0;32m----> 4\u001b[0m     embeddata \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/dill/_dill.py:287\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, ignore, **kwds)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(file, ignore\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m    282\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;124;03m    Unpickle an object from a file.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \n\u001b[1;32m    285\u001b[0m \u001b[38;5;124;03m    See :func:`loads` for keyword arguments.\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mUnpickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/dill/_dill.py:442\u001b[0m, in \u001b[0;36mUnpickler.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m): \u001b[38;5;66;03m#NOTE: if settings change, need to update attributes\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mStockUnpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(obj)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_main_module, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__name__\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    444\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ignore:\n\u001b[1;32m    445\u001b[0m             \u001b[38;5;66;03m# point obj class to main\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/dill/_dill.py:432\u001b[0m, in \u001b[0;36mUnpickler.find_class\u001b[0;34m(self, module, name)\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;66;03m#XXX: special case: NoneType missing\u001b[39;00m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdill.dill\u001b[39m\u001b[38;5;124m'\u001b[39m: module \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdill._dill\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 432\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mStockUnpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers_modules.nomic-ai.nomic-bert-2048.c1b1fd7a715b8eb2e232d34593154ac782c98ac9'"
     ]
    }
   ],
   "source": [
    "import dill as pickle\n",
    "\n",
    "with open(\"data/squad_embedded_full.pickle\", \"rb\") as h:\n",
    "    embeddata = pickle.load(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Database\n",
    "as we have embedded our dataset, we can define a vector database and dump our embeddings in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Qdrant\n",
    "from qdrant_client import models\n",
    "from qdrant_client import QdrantClient\n",
    "class QdrantVDB:\n",
    "    def __init__(self, collection_name, vector_dim=768, batch_size=512):\n",
    "        self.collection_name = collection_name\n",
    "        self.batch_size = batch_size\n",
    "        self.vector_dim = vector_dim\n",
    "    def define_client(self):\n",
    "        self.client = QdrantClient(url=\"http://localhost:6333\",\n",
    "                                   prefer_grpc=True)\n",
    "        \n",
    "    def create_collection(self):\n",
    "        if not self.client.collection_exists(collection_name=self.collection_name):\n",
    "            self.client.create_collection(collection_name=self.collection_name,\n",
    "\n",
    "                                          vectors_config=models.VectorParams(\n",
    "                                                              size=self.vector_dim,\n",
    "                                                              distance=models.Distance.DOT,\n",
    "                                                              on_disk=True),\n",
    "                                          optimizers_config=models.OptimizersConfigDiff(\n",
    "                                                                            default_segment_number=5,\n",
    "                                                                            indexing_threshold=0)\n",
    "                                         )\n",
    "    \n",
    "    def ingest_data(self, embeddata):\n",
    "        for batch_context, batch_embeddings in tqdm(zip(batch_iterate(embeddata.contexts, self.batch_size),\n",
    "                                                        batch_iterate(embeddata.embeddings, self.batch_size)),\n",
    "                                                    total=len(embeddata.contexts)//self.batch_size,\n",
    "                                                    desc = \"Ingesting in batches\"):\n",
    "            self.client.upload_collection(collection_name=self.collection_name,\n",
    "                                          vectors=batch_embeddings,\n",
    "                                          payload=[{\"context\": context} for context in batch_context])\n",
    "            \n",
    "            self.client.update_collection(collection_name=self.collection_name,\n",
    "                                        optimizer_config=models.OptimizersConfigDiff(indexing_threshold=20000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = QdrantVDB(collection_name=\"squad_collection\")\n",
    "database.define_client()\n",
    "database.create_collection()\n",
    "database.ingest_data(embeddata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever \n",
    "## Search and Retrieve from VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Retriever:\n",
    "    def __init__(self, vector_db, embeddata):\n",
    "        self.vector_db = vector_db\n",
    "        self.embeddata = embeddata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(self, query):\n",
    "    query_embedding = self.embeddata.embed_model.get_query_embedding(query)\n",
    "\n",
    "    # Start the timer for logging the time taken for search\n",
    "    start_time = time.time()\n",
    "\n",
    "    result = self.vector_db.client.search()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9(torch cuda)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
