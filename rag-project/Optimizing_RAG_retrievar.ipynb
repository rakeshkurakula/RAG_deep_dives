{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing the RAG performance\n",
    "- better retrievar\n",
    "- better computational load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset preparation\n",
    "- Using Stanford Q and A dataset (SQuAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install datasets\n",
    "# %pip install einops\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# loading the dataset\n",
    "dataset = load_dataset(\"squad\")\n",
    "\n",
    "# Extract unique context from the dataset\n",
    "\n",
    "data = [item[\"context\"] for item in dataset[\"train\"]]\n",
    "\n",
    "texts = list(set(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The United States is the chief remaining nation to assign official responsibilities to a region called the Near East. Within the government the State Department has been most influential in promulgating the Near Eastern regional system. The countries of the former empires of the 19th century have in general abandoned the term and the subdivision in favor of Middle East, North Africa and various forms of Asia. In many cases, such as France, no distinct regional substructures have been employed. Each country has its own French diplomatic apparatus, although regional terms, including Proche-Orient and Moyen-Orient, may be used in a descriptive sense. The most influential agencies in the United States still using Near East as a working concept are as follows.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed dataset\n",
    "Embedding for each context-level, Each element of the above text list will be embedded into a single vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from tqdm import tqdm\n",
    "def batch_iterate(lst, batch_size):\n",
    "    for i in range(0, len(lst), batch_size):\n",
    "        yield lst[i : i + batch_size]\n",
    "    \n",
    "class EmbedData:\n",
    "    \"\"\"\n",
    "    A class for generating and managing text embeddings using a Hugging Face embedding model.\n",
    "    This class handles the loading of an embedding model and batch processing of text data\n",
    "    to generate embeddings.\n",
    "    Attributes:\n",
    "        embed_model_name (str): Name of the Hugging Face model to use for embeddings.\n",
    "            Defaults to \"nomic-ai/nomic-embed-text-v1.5\".\n",
    "        embed_model: Loaded Hugging Face embedding model instance.\n",
    "        batch_size (int): Number of texts to process in each batch. Defaults to 32.\n",
    "        embeddings (list): Storage for generated embeddings.\n",
    "    Example:\n",
    "        >>> embed_data = EmbedData()\n",
    "        >>> texts = [\"Sample text 1\", \"Sample text 2\"]\n",
    "        >>> embed_data.embed(texts)\n",
    "        >>> embeddings = embed_data.embeddings\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 embed_model_name=\"nomic-ai/nomic-embed-text-v1.5\",\n",
    "                 batch_size = 32):\n",
    "        self.embed_model_name = embed_model_name\n",
    "        self.embed_model = self._load_embed_model()\n",
    "        self.batch_size = batch_size\n",
    "        self.embeddings = []\n",
    "\n",
    "    def _load_embed_model(self):\n",
    "        \"\"\"\n",
    "        Load and initialize a HuggingFace embedding model with specified configurations.\n",
    "\n",
    "        Returns:\n",
    "            HuggingFaceEmbedding: Initialized embedding model instance configured with the model name.\n",
    "        \"\"\"\n",
    "        embed_model = HuggingFaceEmbedding(model_name=self.embed_model_name,\n",
    "                                           trust_remote_code=True,\n",
    "                                           cache_folder='./hf_cache')\n",
    "        return embed_model\n",
    "    \n",
    "    def generate_embedding(self, context):\n",
    "        return self.embed_model.get_text_embedding_batch(context)\n",
    "    \n",
    "\n",
    "    def embed(self, contexts):\n",
    "        \"\"\"\n",
    "        Embeds a list of contexts into vector representations using batched processing.\n",
    "        This method processes the input contexts in batches and generates embeddings \n",
    "        for each context using the underlying embedding model. The embeddings are stored\n",
    "        internally in the class instance.\n",
    "        Args:\n",
    "            contexts (list): List of text contexts to be embedded.\n",
    "                             Each context should be a string.\n",
    "        Example:\n",
    "            embedder = EmbeddingModel()\n",
    "            contexts = [\"text1\", \"text2\", \"text3\"]\n",
    "            embedder.embed(contexts)\n",
    "        \"\"\"\n",
    "        self.contexts = contexts\n",
    "        \n",
    "        for batch_context in tqdm(batch_iterate(contexts, self.batch_size),\n",
    "                                  total=len(contexts)//self.batch_size,\n",
    "                                  desc=\"Embedding data in batches\"):\n",
    "                                  \n",
    "            batch_embeddings = self.generate_embedding(batch_context)\n",
    "            \n",
    "            self.embeddings.extend(batch_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rakeshk94/miniconda3/envs/torch/lib/python3.9/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "<All keys matched successfully>\n",
      "Embedding data in batches: 591it [27:37,  2.81s/it]                           \n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "embeddata = EmbedData(batch_size=batch_size)\n",
    "\n",
    "embeddata.embed(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Writing the embedings to pickle file\n",
    "import pickle\n",
    "with open(\"data/squad_embedded_full.pickle\", \"wb\") as h:\n",
    "    pickle.dump(embeddata,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as pickle\n",
    "\n",
    "with open(\"data/squad_embedded_full.pickle\", \"rb\") as h:\n",
    "    embeddata = pickle.load(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Database\n",
    "as we have embedded our dataset, we can define a vector database and dump our embeddings in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Qdrant\n",
    "from qdrant_client import models\n",
    "from qdrant_client import QdrantClient\n",
    "class QdrantVDB:\n",
    "    def __init__(self, collection_name, vector_dim=768, batch_size=512):\n",
    "        self.collection_name = collection_name\n",
    "        self.batch_size = batch_size\n",
    "        self.vector_dim = vector_dim\n",
    "    def define_client(self):\n",
    "        self.client = QdrantClient(url=\"http://localhost:6333\",\n",
    "                                   prefer_grpc=True)\n",
    "        \n",
    "    def create_collection(self):\n",
    "        if not self.client.collection_exists(collection_name=self.collection_name):\n",
    "            self.client.create_collection(collection_name=self.collection_name,\n",
    "\n",
    "                                          vectors_config=models.VectorParams(\n",
    "                                                              size=self.vector_dim,\n",
    "                                                              distance=models.Distance.DOT,\n",
    "                                                              on_disk=True),\n",
    "                                          optimizers_config=models.OptimizersConfigDiff(\n",
    "                                                                            default_segment_number=5,\n",
    "                                                                            indexing_threshold=0),\n",
    "                                            # Adding Binary quantization for faster search\n",
    "                                          quantization_config=models.BinaryQuantization(\n",
    "                                                        binary=models.BinaryQuantizationConfig(always_ram=True)),\n",
    "                                         )\n",
    "    \n",
    "    def ingest_data(self, embeddata):\n",
    "        for batch_context, batch_embeddings in tqdm(zip(batch_iterate(embeddata.contexts, self.batch_size),\n",
    "                                                        batch_iterate(embeddata.embeddings, self.batch_size)),\n",
    "                                                    total=len(embeddata.contexts)//self.batch_size,\n",
    "                                                    desc = \"Ingesting in batches\"):\n",
    "            self.client.upload_collection(collection_name=self.collection_name,\n",
    "                                          vectors=batch_embeddings,\n",
    "                                          payload=[{\"context\": context} for context in batch_context])\n",
    "            \n",
    "            self.client.update_collection(collection_name=self.collection_name,\n",
    "                                        optimizer_config=models.OptimizersConfigDiff(indexing_threshold=20000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ingesting in batches: 37it [00:04,  8.83it/s]                        \n"
     ]
    }
   ],
   "source": [
    "database = QdrantVDB(collection_name=\"squad_collection_qa\")\n",
    "database.define_client()\n",
    "database.create_collection()\n",
    "database.ingest_data(embeddata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever \n",
    "## Search and Retrieve from VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "class Retriever:\n",
    "    def __init__(self, vector_db, embeddata):\n",
    "        self.vector_db = vector_db\n",
    "        self.embeddata = embeddata\n",
    "    def search(self, query):\n",
    "        query_embedding = self.embeddata.embed_model.get_query_embedding(query)\n",
    "\n",
    "        # Start the timer for logging the time taken for search\n",
    "        start_time = time.time()\n",
    "\n",
    "        result = self.vector_db.client.search(\n",
    "            collection_name = self.vector_db.collection_name,\n",
    "            query_vector = query_embedding,\n",
    "            search_params = models.SearchParams(\n",
    "                quantization = models.QuantizationSearchParams(\n",
    "                    ignore= False ,\n",
    "                    rescore = True,\n",
    "                    oversampling = 2.0\n",
    "                )# Adding Ignore as False for quantization\n",
    "            ),\n",
    "            timeout = 1000,\n",
    "        )\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        print(f\"Execution time for the search: {elapsed_time:.4f} seconds\")\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time for the search: 0.0584 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ScoredPoint(id='fa116322-8428-4a95-a4a4-c401f40489a5', version=253, score=0.7405614256858826, payload={'context': 'Paris is located in northern central France. By road it is 450 kilometres (280 mi) south-east of London, 287 kilometres (178 mi) south of Calais, 305 kilometres (190 mi) south-west of Brussels, 774 kilometres (481 mi) north of Marseille, 385 kilometres (239 mi) north-east of Nantes, and 135 kilometres (84 mi) south-east of Rouen. Paris is located in the north-bending arc of the river Seine and includes two islands, the Île Saint-Louis and the larger Île de la Cité, which form the oldest part of the city. The river\\'s mouth on the English Channel (La Manche) is about 233 mi (375 km) downstream of the city, established around 7600 BC. The city is spread widely on both banks of the river. Overall, the city is relatively flat, and the lowest point is 35 m (115 ft) above sea level. Paris has several prominent hills, the highest of which is Montmartre at 130 m (427 ft). Montmartre gained its name from the martyrdom of Saint Denis, first bishop of Paris, atop the Mons Martyrum, \"Martyr\\'s mound\", in 250.'}, vector=None, shard_key=None, order_value=None)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Sample query \n",
    "query = \"What is the capital of France?\"\n",
    "Retriever(database, embeddata).search(query)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Retriever is working well \n",
    "Let's integrate retriever with LLM to generate responses based on retrieved-context and user queries.\n",
    "\n",
    "### Defining RAG - LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "class RAG:\n",
    "    def __init__(self,\n",
    "                 retriever,\n",
    "                 llm_name=\"llama3.2:1b\"):\n",
    "        self.llm_name = llm_name\n",
    "        self.llm = self._setup_llm()\n",
    "        self.retriever = retriever\n",
    "        self.qa_prompt_tmpl_str = \"\"\"Context information is below.\n",
    "                                     ---------------------\n",
    "                                     Context: {context}      \n",
    "                                     ---------------------\n",
    "                                     Given the context information above I want you\n",
    "                                     to think step by step to answer the query in a \n",
    "                                     crisp manner, incase you don't know the \n",
    "                                     answer, please say 'I don't know!'\n",
    "                                     ---------------------\n",
    "                                     Query: {query}\n",
    "                                     ---------------------\n",
    "                                     Answer:\"\"\"\n",
    "\n",
    "    def _setup_llm(self):\n",
    "        return Ollama(model=self.llm_name)\n",
    "    \n",
    "    def generate_context(self, query):\n",
    "        result = self.retriever.search(query)\n",
    "        context = [dict(data) for data in result]\n",
    "        combined_prompt = []\n",
    "        for entry in context:\n",
    "            context = entry[\"payload\"][\"context\"]\n",
    "            combined_prompt.append(context)\n",
    "\n",
    "        return \"\\n\\n---\\n\\n\".join(combined_prompt)\n",
    "    \n",
    "    def query(self, query):\n",
    "        context = self.generate_context(query=query)\n",
    "\n",
    "        prompt = self.qa_prompt_tmpl_str.format(context=context,\n",
    "                                                query=query)\n",
    "        response = self.llm.complete(prompt)\n",
    "        return dict(response)['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrirver = Retriever(database, embeddata)\n",
    "\n",
    "rag = RAG(retriever=retrirver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time for the search: 0.1595 seconds\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"The premium and VIP services in Airports\n",
    "           are reserved for which type of passengers?\"\"\"\n",
    "\n",
    "answer = rag.query(query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Query:** The premium and VIP services in airports are usually reserved for First and Business class passengers."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(f\"**Query:** {str(answer)}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "- We speed up the RAG using Bindary quatization i.e., compute time and memory usage reduction using but at a cost of reduced accuracy\n",
    "- Can explore other quantization like `ScalarQuantization` methods for better output\n",
    "- Binary Quantization usually works on high dimensional datasets.\n",
    "- For smaller datasets, We are better off using traditional RAG.\n",
    "\n",
    "## Limitations\n",
    "- Binary Quantization reduces the precision of the vector embeddings to binary representation, it does lead to a loss of granularity in the original data.\n",
    "- We can incease the precision by oversampling the approximate nearest neighbours.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9(torch cuda)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
